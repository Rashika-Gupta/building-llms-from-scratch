{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.7.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 1.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests>=2.26.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex>=2022.1.18\n",
      "  Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\n",
      "\u001b[K     |████████████████████████████████| 778 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: urllib3<3,>=1.21.1 in /home/rhythm/anaconda3/envs/widgets-tutorial/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (1.25.9)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 16.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /home/rhythm/anaconda3/envs/widgets-tutorial/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2020.6.20)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/rhythm/anaconda3/envs/widgets-tutorial/lib/python3.8/site-packages (from requests>=2.26.0->tiktoken) (2.10)\n",
      "Installing collected packages: charset-normalizer, requests, regex, tiktoken\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.24.0\n",
      "    Uninstalling requests-2.24.0:\n",
      "      Successfully uninstalled requests-2.24.0\n",
      "Successfully installed charset-normalizer-3.3.2 regex-2024.7.24 requests-2.32.3 tiktoken-0.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.0.1\n",
      "tiktoken version: 0.7.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. split text to token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. convert token ids to vectors = embedding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 21935\n",
      "\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a\n",
      "good fellow enough--so it was no\n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "    \n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', '\\n', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no']\n"
     ]
    }
   ],
   "source": [
    "# splitting text to towkn.\n",
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item]\n",
    "print(preprocessed[:38])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 8849\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(preprocessed)) #uniques tokesns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n',\n",
       " ' ',\n",
       " '!',\n",
       " '\"',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '--',\n",
       " '.',\n",
       " '//en',\n",
       " '03',\n",
       " '1',\n",
       " '18',\n",
       " '1908',\n",
       " '1929',\n",
       " '2',\n",
       " '2021',\n",
       " ':',\n",
       " ';',\n",
       " '?',\n",
       " 'A',\n",
       " 'About',\n",
       " 'Add',\n",
       " 'Ah',\n",
       " 'Among',\n",
       " 'And',\n",
       " 'Are',\n",
       " 'Arrt',\n",
       " 'As',\n",
       " 'At',\n",
       " 'Attribution-ShareAlike',\n",
       " 'Be',\n",
       " 'Begin',\n",
       " 'Burlington',\n",
       " 'But',\n",
       " 'By',\n",
       " 'Carlo',\n",
       " 'Categories',\n",
       " 'Chicago',\n",
       " 'Claude',\n",
       " 'Code',\n",
       " 'Come',\n",
       " 'Commons',\n",
       " 'Conduct',\n",
       " 'Cookie',\n",
       " 'Creative',\n",
       " 'Croft',\n",
       " 'DefaultSort',\n",
       " 'Destroyed',\n",
       " 'Developers',\n",
       " 'Devonshire',\n",
       " 'Disclaimers',\n",
       " 'Display',\n",
       " 'Don',\n",
       " 'Dubarry',\n",
       " 'EPUBDownload',\n",
       " 'Emperors',\n",
       " 'English',\n",
       " 'Florence',\n",
       " 'For',\n",
       " 'Gallery',\n",
       " 'Gideon',\n",
       " 'Gisburn',\n",
       " 'Gisburns',\n",
       " 'Grafton',\n",
       " 'Greek',\n",
       " 'Grindle',\n",
       " 'Grindles',\n",
       " 'HAD',\n",
       " 'Had',\n",
       " 'Hang',\n",
       " 'Has',\n",
       " 'He',\n",
       " 'Headers',\n",
       " 'Her',\n",
       " 'Hermia',\n",
       " 'His',\n",
       " 'How',\n",
       " 'I',\n",
       " 'If',\n",
       " 'In',\n",
       " 'It',\n",
       " 'Jack',\n",
       " 'January',\n",
       " 'Jove',\n",
       " 'Just',\n",
       " 'License',\n",
       " 'Lord',\n",
       " 'MOBIDownload',\n",
       " 'Made',\n",
       " 'Main',\n",
       " 'Miss',\n",
       " 'Mobile',\n",
       " 'Money',\n",
       " 'Monte',\n",
       " 'Moon-dancers',\n",
       " 'More',\n",
       " 'Mr',\n",
       " 'Mrs',\n",
       " 'My',\n",
       " 'Namespaces',\n",
       " 'Navigation',\n",
       " 'Never',\n",
       " 'No',\n",
       " 'Not',\n",
       " 'Now',\n",
       " 'Nutley',\n",
       " 'Of',\n",
       " 'Oh',\n",
       " 'On',\n",
       " 'Once',\n",
       " 'Only',\n",
       " 'Options',\n",
       " 'Or',\n",
       " 'PDFOther',\n",
       " 'PageCommunity',\n",
       " 'PageDiscussion',\n",
       " 'Perhaps',\n",
       " 'Personal',\n",
       " 'Policy',\n",
       " 'Poor',\n",
       " 'Print/export',\n",
       " 'Printable',\n",
       " 'Privacy',\n",
       " 'Professional',\n",
       " 'Public',\n",
       " 'QR',\n",
       " 'ReadEditView',\n",
       " 'Renaissance',\n",
       " 'Retrieved',\n",
       " 'Rickham',\n",
       " 'Riviera',\n",
       " 'Rome',\n",
       " 'Russian',\n",
       " 'Search',\n",
       " 'Sevres',\n",
       " 'She',\n",
       " 'States',\n",
       " 'Statistics',\n",
       " 'Stroud',\n",
       " 'Strouds',\n",
       " 'Suddenly',\n",
       " 'Terms',\n",
       " 'Text',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Then',\n",
       " 'There',\n",
       " 'They',\n",
       " 'This',\n",
       " 'Those',\n",
       " 'Though',\n",
       " 'Thwing',\n",
       " 'Thwings',\n",
       " 'To',\n",
       " 'Tools',\n",
       " 'URLDownload',\n",
       " 'United',\n",
       " 'Use',\n",
       " 'Usually',\n",
       " 'Venetian',\n",
       " 'Verdict&oldid=10795836',\n",
       " 'Victor',\n",
       " 'Views',\n",
       " 'Was',\n",
       " 'We',\n",
       " 'Well',\n",
       " 'What',\n",
       " 'When',\n",
       " 'Why',\n",
       " 'Wikisource',\n",
       " 'Yes',\n",
       " 'You',\n",
       " '_',\n",
       " 'a',\n",
       " 'abdication',\n",
       " 'able',\n",
       " 'about',\n",
       " 'above',\n",
       " 'abruptly',\n",
       " 'absolute',\n",
       " 'absorbed',\n",
       " 'absurdity',\n",
       " 'academic',\n",
       " 'accountLog',\n",
       " 'accuse',\n",
       " 'accustomed',\n",
       " 'across',\n",
       " 'activity',\n",
       " 'add',\n",
       " 'added',\n",
       " 'additional',\n",
       " 'admirers',\n",
       " 'adopted',\n",
       " 'adulation',\n",
       " 'advance',\n",
       " 'aesthetic',\n",
       " 'affect',\n",
       " 'afraid',\n",
       " 'after',\n",
       " 'afterward',\n",
       " 'again',\n",
       " 'ago',\n",
       " 'agree',\n",
       " 'ah',\n",
       " 'air',\n",
       " 'alive',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'always',\n",
       " 'am',\n",
       " 'amazement',\n",
       " 'amid',\n",
       " 'among',\n",
       " 'amplest',\n",
       " 'amusing',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'answer',\n",
       " 'answered',\n",
       " 'any',\n",
       " 'anything',\n",
       " 'anywhere',\n",
       " 'apparent',\n",
       " 'apparently',\n",
       " 'appearance',\n",
       " 'appeared',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'appointed',\n",
       " 'are',\n",
       " 'areas',\n",
       " 'arm',\n",
       " 'arm-chair',\n",
       " 'arm-chairs',\n",
       " 'arms',\n",
       " 'art',\n",
       " 'articles',\n",
       " 'artist',\n",
       " 'as',\n",
       " 'aside',\n",
       " 'asked',\n",
       " 'at',\n",
       " 'atmosphere',\n",
       " 'atom',\n",
       " 'attack',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'audacities',\n",
       " 'authorRandom',\n",
       " 'available',\n",
       " 'away',\n",
       " 'awful',\n",
       " 'axioms',\n",
       " 'azaleas',\n",
       " 'back',\n",
       " 'background',\n",
       " 'balance',\n",
       " 'balancing',\n",
       " 'balustraded',\n",
       " 'basking',\n",
       " 'bath-rooms',\n",
       " 'be',\n",
       " 'beaming',\n",
       " 'bean-stalk',\n",
       " 'bear',\n",
       " 'beard',\n",
       " 'beauty',\n",
       " 'became',\n",
       " 'because',\n",
       " 'becoming',\n",
       " 'bed',\n",
       " 'been',\n",
       " 'before',\n",
       " 'began',\n",
       " 'begun',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'believed',\n",
       " 'beneath',\n",
       " 'bespoke',\n",
       " 'better',\n",
       " 'between',\n",
       " 'big',\n",
       " 'bits',\n",
       " 'bitterness',\n",
       " 'blocked',\n",
       " 'born',\n",
       " 'borne',\n",
       " 'boudoir',\n",
       " 'bravura',\n",
       " 'break',\n",
       " 'breaking',\n",
       " 'breathing',\n",
       " 'bric-a-brac',\n",
       " 'briefly',\n",
       " 'brings',\n",
       " 'bronzes',\n",
       " 'brought',\n",
       " 'brown',\n",
       " 'brush',\n",
       " 'bull',\n",
       " 'business',\n",
       " 'but',\n",
       " 'buying',\n",
       " 'by',\n",
       " 'called',\n",
       " 'came',\n",
       " 'can',\n",
       " 'canvas',\n",
       " 'canvases',\n",
       " 'cards',\n",
       " 'care',\n",
       " 'career',\n",
       " 'category',\n",
       " 'caught',\n",
       " 'central',\n",
       " 'chair',\n",
       " 'changesSpecial',\n",
       " 'changesSubject',\n",
       " 'chap',\n",
       " 'characteristic',\n",
       " 'charming',\n",
       " 'cheap',\n",
       " 'check',\n",
       " 'cheeks',\n",
       " 'chest',\n",
       " 'chimney-piece',\n",
       " 'chucked',\n",
       " 'cigar',\n",
       " 'cigarette',\n",
       " 'cigars',\n",
       " 'circulation',\n",
       " 'circumstance',\n",
       " 'circus-clown',\n",
       " 'claimed',\n",
       " 'clasping',\n",
       " 'clear',\n",
       " 'cleverer',\n",
       " 'close',\n",
       " 'clue',\n",
       " 'coat',\n",
       " 'code',\n",
       " 'collapsed',\n",
       " 'colour',\n",
       " 'come',\n",
       " 'comfortable',\n",
       " 'coming',\n",
       " 'companion',\n",
       " 'compared',\n",
       " 'complex',\n",
       " 'confident',\n",
       " 'congesting',\n",
       " 'conjugal',\n",
       " 'constraint',\n",
       " 'consummate',\n",
       " 'contended',\n",
       " 'continued',\n",
       " 'copyright',\n",
       " 'corner',\n",
       " 'corrected',\n",
       " 'could',\n",
       " 'couldn',\n",
       " 'count',\n",
       " 'countenance',\n",
       " 'countries',\n",
       " 'couple',\n",
       " 'course',\n",
       " 'covered',\n",
       " 'craft',\n",
       " 'cried',\n",
       " 'crossed',\n",
       " 'crowned',\n",
       " 'crumbled',\n",
       " 'cry',\n",
       " 'cured',\n",
       " 'curiosity',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'curtains',\n",
       " 'd',\n",
       " 'dabble',\n",
       " 'damask',\n",
       " 'dark',\n",
       " 'dashed',\n",
       " 'day',\n",
       " 'days',\n",
       " 'dead',\n",
       " 'deadening',\n",
       " 'dear',\n",
       " 'deep',\n",
       " 'deerhound',\n",
       " 'degree',\n",
       " 'delicate',\n",
       " 'demand',\n",
       " 'denied',\n",
       " 'deploring',\n",
       " 'deprecating',\n",
       " 'deprecatingly',\n",
       " 'desire',\n",
       " 'destroyed',\n",
       " 'destruction',\n",
       " 'desultory',\n",
       " 'detail',\n",
       " 'diagnosis',\n",
       " 'did',\n",
       " 'didn',\n",
       " 'died',\n",
       " 'dim',\n",
       " 'dimmest',\n",
       " 'dingy',\n",
       " 'dining-room',\n",
       " 'disarming',\n",
       " 'discovery',\n",
       " 'discrimination',\n",
       " 'discussion',\n",
       " 'discussionRecent',\n",
       " 'disdain',\n",
       " 'disdained',\n",
       " 'disease',\n",
       " 'disguised',\n",
       " 'display',\n",
       " 'dissatisfied',\n",
       " 'distinguished',\n",
       " 'distract',\n",
       " 'divert',\n",
       " 'do',\n",
       " 'doesn',\n",
       " 'doing',\n",
       " 'domain',\n",
       " 'domainPublic',\n",
       " 'domainfalsefalse',\n",
       " 'domestic',\n",
       " 'don',\n",
       " 'done',\n",
       " 'donkey',\n",
       " 'down',\n",
       " 'dozen',\n",
       " 'dragged',\n",
       " 'drawing-room',\n",
       " 'drawing-rooms',\n",
       " 'drawn',\n",
       " 'dress-closets',\n",
       " 'drew',\n",
       " 'dropped',\n",
       " 'each',\n",
       " 'earth',\n",
       " 'ease',\n",
       " 'easel',\n",
       " 'easy',\n",
       " 'echoed',\n",
       " 'economy',\n",
       " 'edited',\n",
       " 'effect',\n",
       " 'effects',\n",
       " 'efforts',\n",
       " 'egregious',\n",
       " 'eighteenth-century',\n",
       " 'elbow',\n",
       " 'elegant',\n",
       " 'else',\n",
       " 'embarrassed',\n",
       " 'enabled',\n",
       " 'end',\n",
       " 'endless',\n",
       " 'enjoy',\n",
       " 'enlightenment',\n",
       " 'enough',\n",
       " 'ensuing',\n",
       " 'equally',\n",
       " 'equanimity',\n",
       " 'escape',\n",
       " 'established',\n",
       " 'etching',\n",
       " 'even',\n",
       " 'event',\n",
       " 'ever',\n",
       " 'everlasting',\n",
       " 'every',\n",
       " 'exasperated',\n",
       " 'except',\n",
       " 'excuse',\n",
       " 'excusing',\n",
       " 'existed',\n",
       " 'expected',\n",
       " 'exquisite',\n",
       " 'exquisitely',\n",
       " 'extenuation',\n",
       " 'exterminating',\n",
       " 'extracting',\n",
       " 'eye',\n",
       " 'eyebrows',\n",
       " 'eyes',\n",
       " 'face',\n",
       " 'faces',\n",
       " 'fact',\n",
       " 'faded',\n",
       " 'failed',\n",
       " 'failure',\n",
       " 'fair',\n",
       " 'faith',\n",
       " 'false',\n",
       " 'familiar',\n",
       " 'famille-verte',\n",
       " 'fancy',\n",
       " 'fashionable',\n",
       " 'fate',\n",
       " 'feather',\n",
       " 'feet',\n",
       " 'fell',\n",
       " 'fellow',\n",
       " 'felt',\n",
       " 'few',\n",
       " 'fewer',\n",
       " 'finality',\n",
       " 'find',\n",
       " 'fingers',\n",
       " 'first',\n",
       " 'fit',\n",
       " 'fitting',\n",
       " 'five',\n",
       " 'flash',\n",
       " 'flashed',\n",
       " 'florid',\n",
       " 'flowers',\n",
       " 'fluently',\n",
       " 'flung',\n",
       " 'follow',\n",
       " 'followed',\n",
       " 'fond',\n",
       " 'footstep',\n",
       " 'for',\n",
       " 'forced',\n",
       " 'forcing',\n",
       " 'forehead',\n",
       " 'foreign',\n",
       " 'foreseen',\n",
       " 'forgive',\n",
       " 'forgotten',\n",
       " 'form',\n",
       " 'formats',\n",
       " 'formed',\n",
       " 'forming',\n",
       " 'forward',\n",
       " 'fostered',\n",
       " 'found',\n",
       " 'foundations',\n",
       " 'fragment',\n",
       " 'fragments',\n",
       " 'frame',\n",
       " 'frames',\n",
       " 'frequently',\n",
       " 'friend',\n",
       " 'from',\n",
       " 'full',\n",
       " 'fullest',\n",
       " 'furiously',\n",
       " 'furrowed',\n",
       " 'garlanded',\n",
       " 'garlands',\n",
       " 'gave',\n",
       " 'genial',\n",
       " 'genius',\n",
       " 'gesture',\n",
       " 'get',\n",
       " 'getting',\n",
       " 'give',\n",
       " 'given',\n",
       " 'glad',\n",
       " 'glanced',\n",
       " 'glimpse',\n",
       " 'gloried',\n",
       " 'glory',\n",
       " 'go',\n",
       " 'going',\n",
       " 'gone',\n",
       " 'good',\n",
       " 'good-breeding',\n",
       " 'good-humoured',\n",
       " 'got',\n",
       " 'grace',\n",
       " 'gradually',\n",
       " 'gray',\n",
       " 'grayish',\n",
       " 'great',\n",
       " 'greatest',\n",
       " 'greatness',\n",
       " 'grew',\n",
       " 'groping',\n",
       " 'growing',\n",
       " 'had',\n",
       " 'hadn',\n",
       " 'hair',\n",
       " 'half',\n",
       " 'half-light',\n",
       " 'half-mechanically',\n",
       " 'hall',\n",
       " 'hand',\n",
       " 'hands',\n",
       " 'handsome',\n",
       " 'hanging',\n",
       " 'happen',\n",
       " 'happened',\n",
       " 'hard',\n",
       " 'hardly',\n",
       " 'has',\n",
       " 'have',\n",
       " 'haven',\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'hear',\n",
       " 'heard',\n",
       " 'heart',\n",
       " 'height',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereRelated',\n",
       " 'hermit',\n",
       " 'herself',\n",
       " 'hesitations',\n",
       " 'hide',\n",
       " 'high',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'hint',\n",
       " 'his',\n",
       " 'history',\n",
       " 'holding',\n",
       " 'home',\n",
       " 'honour',\n",
       " 'hooded',\n",
       " 'hostess',\n",
       " 'hot-house',\n",
       " 'hour',\n",
       " 'hours',\n",
       " 'house',\n",
       " 'how',\n",
       " 'https',\n",
       " 'hung',\n",
       " 'husband',\n",
       " 'idea',\n",
       " 'idle',\n",
       " 'idling',\n",
       " 'if',\n",
       " 'immediately',\n",
       " 'in',\n",
       " 'inTalkContributionsCreate',\n",
       " 'incense',\n",
       " 'indexAuthorsRandom',\n",
       " 'indifferent',\n",
       " 'inevitable',\n",
       " 'inevitably',\n",
       " 'inflexible',\n",
       " 'informationCite',\n",
       " 'insensible',\n",
       " 'insignificant',\n",
       " 'instinctively',\n",
       " 'instructive',\n",
       " 'interesting',\n",
       " 'into',\n",
       " 'ironic',\n",
       " 'irony',\n",
       " 'irrelevance',\n",
       " 'irrevocable',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'jardiniere',\n",
       " 'jealousy',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'kept',\n",
       " 'key',\n",
       " 'kind',\n",
       " 'knees',\n",
       " 'knew',\n",
       " 'know',\n",
       " 'known',\n",
       " 'laid',\n",
       " 'lair',\n",
       " 'landing',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'last',\n",
       " 'late',\n",
       " 'later',\n",
       " 'latter',\n",
       " 'laugh',\n",
       " 'laughed',\n",
       " 'lay',\n",
       " 'leading',\n",
       " 'lean',\n",
       " 'learned',\n",
       " 'least',\n",
       " 'leathery',\n",
       " 'leave',\n",
       " 'led',\n",
       " 'left',\n",
       " 'leisure',\n",
       " 'lends',\n",
       " 'lent',\n",
       " 'let',\n",
       " 'lies',\n",
       " 'life',\n",
       " 'life-likeness',\n",
       " 'lift',\n",
       " 'lifted',\n",
       " 'light',\n",
       " 'lightly',\n",
       " 'like',\n",
       " 'liked',\n",
       " 'line',\n",
       " 'lines',\n",
       " 'lingered',\n",
       " 'linkPage',\n",
       " 'links',\n",
       " 'lips',\n",
       " 'lit',\n",
       " 'little',\n",
       " 'live',\n",
       " 'll',\n",
       " 'loathing',\n",
       " 'logged',\n",
       " 'long',\n",
       " 'longed',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'looked',\n",
       " 'looking',\n",
       " 'lose',\n",
       " 'loss',\n",
       " 'lounging',\n",
       " 'lovely',\n",
       " 'lucky',\n",
       " 'lump',\n",
       " 'luncheon-table',\n",
       " 'luxury',\n",
       " 'lying',\n",
       " 'made',\n",
       " 'make',\n",
       " 'man',\n",
       " 'manage',\n",
       " 'managed',\n",
       " 'mantel-piece',\n",
       " 'marble',\n",
       " 'married',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meant',\n",
       " 'mediocrity',\n",
       " 'medium',\n",
       " 'mentioned',\n",
       " 'menu',\n",
       " 'mere',\n",
       " 'merely',\n",
       " 'met',\n",
       " 'might',\n",
       " 'mighty',\n",
       " 'millionaire',\n",
       " 'mine',\n",
       " 'minute',\n",
       " 'minutes',\n",
       " 'mirrors',\n",
       " 'modest',\n",
       " 'modesty',\n",
       " 'moment',\n",
       " 'money',\n",
       " 'monumental',\n",
       " 'mood',\n",
       " 'morbidly',\n",
       " 'more',\n",
       " 'most',\n",
       " 'mourn',\n",
       " 'mourned',\n",
       " 'moustache',\n",
       " 'moved',\n",
       " 'much',\n",
       " 'muddling',\n",
       " 'multiplied',\n",
       " 'murmur',\n",
       " 'muscles',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'mysterious',\n",
       " 'naive',\n",
       " 'native',\n",
       " 'near',\n",
       " 'nearly',\n",
       " 'negatived',\n",
       " 'nervous',\n",
       " 'nervousness',\n",
       " 'neutral',\n",
       " 'never',\n",
       " 'next',\n",
       " 'no',\n",
       " 'none',\n",
       " 'not',\n",
       " 'note',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nymphs',\n",
       " 'oak',\n",
       " 'obituary',\n",
       " 'object',\n",
       " 'objects',\n",
       " 'occurred',\n",
       " 'oddly',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'oh',\n",
       " 'old',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'ones',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'open',\n",
       " 'or',\n",
       " 'org/w/index',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'outline',\n",
       " 'oval',\n",
       " 'over',\n",
       " 'own',\n",
       " 'packed',\n",
       " 'page',\n",
       " 'pageGet',\n",
       " 'pagesPermanent',\n",
       " 'paid',\n",
       " 'paint',\n",
       " 'painted',\n",
       " 'painter',\n",
       " 'painting',\n",
       " 'pale',\n",
       " 'paled',\n",
       " 'palm-trees',\n",
       " 'panel',\n",
       " 'panelling',\n",
       " 'pardonable',\n",
       " 'pardoned',\n",
       " 'part',\n",
       " 'passages',\n",
       " 'passing',\n",
       " 'past',\n",
       " 'pastels',\n",
       " 'pathos',\n",
       " 'patient',\n",
       " 'people',\n",
       " 'perceptible',\n",
       " 'perfect',\n",
       " 'persistence',\n",
       " 'persuasively',\n",
       " 'php',\n",
       " 'phrase',\n",
       " 'picture',\n",
       " 'pictures',\n",
       " 'pines',\n",
       " 'pink',\n",
       " 'place',\n",
       " 'placed',\n",
       " 'plain',\n",
       " 'platitudes',\n",
       " 'pleased',\n",
       " 'pockets',\n",
       " 'point',\n",
       " 'poised',\n",
       " 'policy',\n",
       " 'poor',\n",
       " 'portalCentral',\n",
       " 'portrait',\n",
       " 'posing',\n",
       " 'possessed',\n",
       " 'poverty',\n",
       " 'predicted',\n",
       " 'preliminary',\n",
       " 'presenting',\n",
       " 'prestidigitation',\n",
       " 'pretty',\n",
       " 'previous',\n",
       " 'price',\n",
       " 'pride',\n",
       " 'princely',\n",
       " 'prism',\n",
       " 'problem',\n",
       " 'proclaiming',\n",
       " 'prodigious',\n",
       " 'profusion',\n",
       " 'projects',\n",
       " 'protest',\n",
       " 'prove',\n",
       " 'public',\n",
       " 'published',\n",
       " 'purblind',\n",
       " 'purely',\n",
       " 'pushed',\n",
       " 'put',\n",
       " 'qualities',\n",
       " 'quality',\n",
       " 'queerly',\n",
       " 'question',\n",
       " 'quickly',\n",
       " 'quietly',\n",
       " 'quite',\n",
       " 'quote',\n",
       " 'rain',\n",
       " 'raised',\n",
       " 'random',\n",
       " 'rather',\n",
       " 're',\n",
       " 'real',\n",
       " 'really',\n",
       " 'reared',\n",
       " 'reason',\n",
       " 'reassurance',\n",
       " 'recovering',\n",
       " 'recreated',\n",
       " 'reflected',\n",
       " 'reflection',\n",
       " 'regrets',\n",
       " 'relatively',\n",
       " 'remained',\n",
       " 'remember',\n",
       " 'reminded',\n",
       " 'repeating',\n",
       " 'represented',\n",
       " 'reproduction',\n",
       " 'resented',\n",
       " 'resolve',\n",
       " 'resources',\n",
       " 'rest',\n",
       " 'rich',\n",
       " 'ridiculous',\n",
       " 'robbed',\n",
       " 'romantic',\n",
       " 'room',\n",
       " 'rose',\n",
       " 'rs',\n",
       " 'rule',\n",
       " 'run',\n",
       " 's',\n",
       " 'said',\n",
       " 'same',\n",
       " 'satisfaction',\n",
       " 'savour',\n",
       " 'saw',\n",
       " 'say',\n",
       " 'saying',\n",
       " 'says',\n",
       " 'scorn',\n",
       " 'scornful',\n",
       " 'secret',\n",
       " 'see',\n",
       " 'seemed',\n",
       " 'seen',\n",
       " 'self-confident',\n",
       " 'send',\n",
       " 'sensation',\n",
       " 'sensitive',\n",
       " 'sent',\n",
       " 'serious',\n",
       " 'set',\n",
       " 'sex',\n",
       " 'shade',\n",
       " 'shaking',\n",
       " 'shall',\n",
       " 'she',\n",
       " 'shirked',\n",
       " 'short',\n",
       " 'shortened',\n",
       " 'shorter',\n",
       " 'should',\n",
       " 'shoulder',\n",
       " 'shoulders',\n",
       " 'show',\n",
       " 'showed',\n",
       " 'showy',\n",
       " 'shrug',\n",
       " 'shrugged',\n",
       " 'sight',\n",
       " 'sign',\n",
       " 'silent',\n",
       " 'silver',\n",
       " 'similar',\n",
       " 'simpleton',\n",
       " 'simplifications',\n",
       " 'simply',\n",
       " 'since',\n",
       " 'single',\n",
       " ...]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(set(preprocessed)) #not ccont"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '\\n')\n",
      "(1, ' ')\n",
      "(2, '!')\n",
      "(3, '\"')\n",
      "(4, \"'\")\n",
      "(5, '(')\n",
      "(6, ')')\n",
      "(7, ',')\n",
      "(8, '--')\n",
      "(9, '.')\n",
      "(10, '//en')\n",
      "(11, '03')\n",
      "(12, '1')\n",
      "(13, '18')\n",
      "(14, '1908')\n",
      "(15, '1929')\n",
      "(16, '2')\n",
      "(17, '2021')\n",
      "(18, ':')\n",
      "(19, ';')\n",
      "(20, '?')\n",
      "(21, 'A')\n",
      "(22, 'About')\n",
      "(23, 'Add')\n",
      "(24, 'Ah')\n",
      "(25, 'Among')\n",
      "(26, 'And')\n",
      "(27, 'Are')\n",
      "(28, 'Arrt')\n",
      "(29, 'As')\n",
      "(30, 'At')\n",
      "(31, 'Attribution-ShareAlike')\n",
      "(32, 'Be')\n",
      "(33, 'Begin')\n",
      "(34, 'Burlington')\n",
      "(35, 'But')\n",
      "(36, 'By')\n",
      "(37, 'Carlo')\n",
      "(38, 'Categories')\n",
      "(39, 'Chicago')\n",
      "(40, 'Claude')\n",
      "(41, 'Code')\n",
      "(42, 'Come')\n",
      "(43, 'Commons')\n",
      "(44, 'Conduct')\n",
      "(45, 'Cookie')\n",
      "(46, 'Creative')\n",
      "(47, 'Croft')\n",
      "(48, 'DefaultSort')\n",
      "(49, 'Destroyed')\n",
      "(50, 'Developers')\n",
      "(51, 'Devonshire')\n",
      "(52, 'Disclaimers')\n",
      "(53, 'Display')\n",
      "(54, 'Don')\n",
      "(55, 'Dubarry')\n",
      "(56, 'EPUBDownload')\n",
      "(57, 'Emperors')\n",
      "(58, 'English')\n",
      "(59, 'Florence')\n",
      "(60, 'For')\n",
      "(61, 'Gallery')\n",
      "(62, 'Gideon')\n",
      "(63, 'Gisburn')\n",
      "(64, 'Gisburns')\n",
      "(65, 'Grafton')\n",
      "(66, 'Greek')\n",
      "(67, 'Grindle')\n",
      "(68, 'Grindles')\n",
      "(69, 'HAD')\n",
      "(70, 'Had')\n",
      "(71, 'Hang')\n",
      "(72, 'Has')\n",
      "(73, 'He')\n",
      "(74, 'Headers')\n",
      "(75, 'Her')\n",
      "(76, 'Hermia')\n",
      "(77, 'His')\n",
      "(78, 'How')\n",
      "(79, 'I')\n",
      "(80, 'If')\n",
      "(81, 'In')\n",
      "(82, 'It')\n",
      "(83, 'Jack')\n",
      "(84, 'January')\n",
      "(85, 'Jove')\n",
      "(86, 'Just')\n",
      "(87, 'License')\n",
      "(88, 'Lord')\n",
      "(89, 'MOBIDownload')\n",
      "(90, 'Made')\n",
      "(91, 'Main')\n",
      "(92, 'Miss')\n",
      "(93, 'Mobile')\n",
      "(94, 'Money')\n",
      "(95, 'Monte')\n",
      "(96, 'Moon-dancers')\n",
      "(97, 'More')\n",
      "(98, 'Mr')\n",
      "(99, 'Mrs')\n",
      "(100, 'My')\n",
      "(101, 'Namespaces')\n",
      "(102, 'Navigation')\n",
      "(103, 'Never')\n",
      "(104, 'No')\n",
      "(105, 'Not')\n",
      "(106, 'Now')\n",
      "(107, 'Nutley')\n",
      "(108, 'Of')\n",
      "(109, 'Oh')\n",
      "(110, 'On')\n",
      "(111, 'Once')\n",
      "(112, 'Only')\n",
      "(113, 'Options')\n",
      "(114, 'Or')\n",
      "(115, 'PDFOther')\n",
      "(116, 'PageCommunity')\n",
      "(117, 'PageDiscussion')\n",
      "(118, 'Perhaps')\n",
      "(119, 'Personal')\n",
      "(120, 'Policy')\n",
      "(121, 'Poor')\n",
      "(122, 'Print/export')\n",
      "(123, 'Printable')\n",
      "(124, 'Privacy')\n",
      "(125, 'Professional')\n",
      "(126, 'Public')\n",
      "(127, 'QR')\n",
      "(128, 'ReadEditView')\n",
      "(129, 'Renaissance')\n",
      "(130, 'Retrieved')\n",
      "(131, 'Rickham')\n",
      "(132, 'Riviera')\n",
      "(133, 'Rome')\n",
      "(134, 'Russian')\n",
      "(135, 'Search')\n",
      "(136, 'Sevres')\n",
      "(137, 'She')\n",
      "(138, 'States')\n",
      "(139, 'Statistics')\n",
      "(140, 'Stroud')\n",
      "(141, 'Strouds')\n",
      "(142, 'Suddenly')\n",
      "(143, 'Terms')\n",
      "(144, 'Text')\n",
      "(145, 'That')\n",
      "(146, 'The')\n",
      "(147, 'Then')\n",
      "(148, 'There')\n",
      "(149, 'They')\n",
      "(150, 'This')\n",
      "(151, 'Those')\n",
      "(152, 'Though')\n",
      "(153, 'Thwing')\n",
      "(154, 'Thwings')\n",
      "(155, 'To')\n",
      "(156, 'Tools')\n",
      "(157, 'URLDownload')\n",
      "(158, 'United')\n",
      "(159, 'Use')\n",
      "(160, 'Usually')\n",
      "(161, 'Venetian')\n",
      "(162, 'Verdict&oldid=10795836')\n",
      "(163, 'Victor')\n",
      "(164, 'Views')\n",
      "(165, 'Was')\n",
      "(166, 'We')\n",
      "(167, 'Well')\n",
      "(168, 'What')\n",
      "(169, 'When')\n",
      "(170, 'Why')\n",
      "(171, 'Wikisource')\n",
      "(172, 'Yes')\n",
      "(173, 'You')\n",
      "(174, '_')\n",
      "(175, 'a')\n",
      "(176, 'abdication')\n",
      "(177, 'able')\n",
      "(178, 'about')\n",
      "(179, 'above')\n",
      "(180, 'abruptly')\n",
      "(181, 'absolute')\n",
      "(182, 'absorbed')\n",
      "(183, 'absurdity')\n",
      "(184, 'academic')\n",
      "(185, 'accountLog')\n",
      "(186, 'accuse')\n",
      "(187, 'accustomed')\n",
      "(188, 'across')\n",
      "(189, 'activity')\n",
      "(190, 'add')\n",
      "(191, 'added')\n",
      "(192, 'additional')\n",
      "(193, 'admirers')\n",
      "(194, 'adopted')\n",
      "(195, 'adulation')\n",
      "(196, 'advance')\n",
      "(197, 'aesthetic')\n",
      "(198, 'affect')\n",
      "(199, 'afraid')\n",
      "(200, 'after')\n",
      "(201, 'afterward')\n",
      "(202, 'again')\n",
      "(203, 'ago')\n",
      "(204, 'agree')\n",
      "(205, 'ah')\n",
      "(206, 'air')\n",
      "(207, 'alive')\n",
      "(208, 'all')\n",
      "(209, 'almost')\n",
      "(210, 'alone')\n",
      "(211, 'along')\n",
      "(212, 'always')\n",
      "(213, 'am')\n",
      "(214, 'amazement')\n",
      "(215, 'amid')\n",
      "(216, 'among')\n",
      "(217, 'amplest')\n",
      "(218, 'amusing')\n",
      "(219, 'an')\n",
      "(220, 'and')\n",
      "(221, 'another')\n",
      "(222, 'answer')\n",
      "(223, 'answered')\n",
      "(224, 'any')\n",
      "(225, 'anything')\n",
      "(226, 'anywhere')\n",
      "(227, 'apparent')\n",
      "(228, 'apparently')\n",
      "(229, 'appearance')\n",
      "(230, 'appeared')\n",
      "(231, 'apply')\n",
      "(232, 'applying')\n",
      "(233, 'appointed')\n",
      "(234, 'are')\n",
      "(235, 'areas')\n",
      "(236, 'arm')\n",
      "(237, 'arm-chair')\n",
      "(238, 'arm-chairs')\n",
      "(239, 'arms')\n",
      "(240, 'art')\n",
      "(241, 'articles')\n",
      "(242, 'artist')\n",
      "(243, 'as')\n",
      "(244, 'aside')\n",
      "(245, 'asked')\n",
      "(246, 'at')\n",
      "(247, 'atmosphere')\n",
      "(248, 'atom')\n",
      "(249, 'attack')\n",
      "(250, 'attention')\n",
      "(251, 'attitude')\n",
      "(252, 'audacities')\n",
      "(253, 'authorRandom')\n",
      "(254, 'available')\n",
      "(255, 'away')\n",
      "(256, 'awful')\n",
      "(257, 'axioms')\n",
      "(258, 'azaleas')\n",
      "(259, 'back')\n",
      "(260, 'background')\n",
      "(261, 'balance')\n",
      "(262, 'balancing')\n",
      "(263, 'balustraded')\n",
      "(264, 'basking')\n",
      "(265, 'bath-rooms')\n",
      "(266, 'be')\n",
      "(267, 'beaming')\n",
      "(268, 'bean-stalk')\n",
      "(269, 'bear')\n",
      "(270, 'beard')\n",
      "(271, 'beauty')\n",
      "(272, 'became')\n",
      "(273, 'because')\n",
      "(274, 'becoming')\n",
      "(275, 'bed')\n",
      "(276, 'been')\n",
      "(277, 'before')\n",
      "(278, 'began')\n",
      "(279, 'begun')\n",
      "(280, 'behind')\n",
      "(281, 'being')\n",
      "(282, 'believed')\n",
      "(283, 'beneath')\n",
      "(284, 'bespoke')\n",
      "(285, 'better')\n",
      "(286, 'between')\n",
      "(287, 'big')\n",
      "(288, 'bits')\n",
      "(289, 'bitterness')\n",
      "(290, 'blocked')\n",
      "(291, 'born')\n",
      "(292, 'borne')\n",
      "(293, 'boudoir')\n",
      "(294, 'bravura')\n",
      "(295, 'break')\n",
      "(296, 'breaking')\n",
      "(297, 'breathing')\n",
      "(298, 'bric-a-brac')\n",
      "(299, 'briefly')\n",
      "(300, 'brings')\n",
      "(301, 'bronzes')\n",
      "(302, 'brought')\n",
      "(303, 'brown')\n",
      "(304, 'brush')\n",
      "(305, 'bull')\n",
      "(306, 'business')\n",
      "(307, 'but')\n",
      "(308, 'buying')\n",
      "(309, 'by')\n",
      "(310, 'called')\n",
      "(311, 'came')\n",
      "(312, 'can')\n",
      "(313, 'canvas')\n",
      "(314, 'canvases')\n",
      "(315, 'cards')\n",
      "(316, 'care')\n",
      "(317, 'career')\n",
      "(318, 'category')\n",
      "(319, 'caught')\n",
      "(320, 'central')\n",
      "(321, 'chair')\n",
      "(322, 'changesSpecial')\n",
      "(323, 'changesSubject')\n",
      "(324, 'chap')\n",
      "(325, 'characteristic')\n",
      "(326, 'charming')\n",
      "(327, 'cheap')\n",
      "(328, 'check')\n",
      "(329, 'cheeks')\n",
      "(330, 'chest')\n",
      "(331, 'chimney-piece')\n",
      "(332, 'chucked')\n",
      "(333, 'cigar')\n",
      "(334, 'cigarette')\n",
      "(335, 'cigars')\n",
      "(336, 'circulation')\n",
      "(337, 'circumstance')\n",
      "(338, 'circus-clown')\n",
      "(339, 'claimed')\n",
      "(340, 'clasping')\n",
      "(341, 'clear')\n",
      "(342, 'cleverer')\n",
      "(343, 'close')\n",
      "(344, 'clue')\n",
      "(345, 'coat')\n",
      "(346, 'code')\n",
      "(347, 'collapsed')\n",
      "(348, 'colour')\n",
      "(349, 'come')\n",
      "(350, 'comfortable')\n",
      "(351, 'coming')\n",
      "(352, 'companion')\n",
      "(353, 'compared')\n",
      "(354, 'complex')\n",
      "(355, 'confident')\n",
      "(356, 'congesting')\n",
      "(357, 'conjugal')\n",
      "(358, 'constraint')\n",
      "(359, 'consummate')\n",
      "(360, 'contended')\n",
      "(361, 'continued')\n",
      "(362, 'copyright')\n",
      "(363, 'corner')\n",
      "(364, 'corrected')\n",
      "(365, 'could')\n",
      "(366, 'couldn')\n",
      "(367, 'count')\n",
      "(368, 'countenance')\n",
      "(369, 'countries')\n",
      "(370, 'couple')\n",
      "(371, 'course')\n",
      "(372, 'covered')\n",
      "(373, 'craft')\n",
      "(374, 'cried')\n",
      "(375, 'crossed')\n",
      "(376, 'crowned')\n",
      "(377, 'crumbled')\n",
      "(378, 'cry')\n",
      "(379, 'cured')\n",
      "(380, 'curiosity')\n",
      "(381, 'curious')\n",
      "(382, 'current')\n",
      "(383, 'curtains')\n",
      "(384, 'd')\n",
      "(385, 'dabble')\n",
      "(386, 'damask')\n",
      "(387, 'dark')\n",
      "(388, 'dashed')\n",
      "(389, 'day')\n",
      "(390, 'days')\n",
      "(391, 'dead')\n",
      "(392, 'deadening')\n",
      "(393, 'dear')\n",
      "(394, 'deep')\n",
      "(395, 'deerhound')\n",
      "(396, 'degree')\n",
      "(397, 'delicate')\n",
      "(398, 'demand')\n",
      "(399, 'denied')\n",
      "(400, 'deploring')\n",
      "(401, 'deprecating')\n",
      "(402, 'deprecatingly')\n",
      "(403, 'desire')\n",
      "(404, 'destroyed')\n",
      "(405, 'destruction')\n",
      "(406, 'desultory')\n",
      "(407, 'detail')\n",
      "(408, 'diagnosis')\n",
      "(409, 'did')\n",
      "(410, 'didn')\n",
      "(411, 'died')\n",
      "(412, 'dim')\n",
      "(413, 'dimmest')\n",
      "(414, 'dingy')\n",
      "(415, 'dining-room')\n",
      "(416, 'disarming')\n",
      "(417, 'discovery')\n",
      "(418, 'discrimination')\n",
      "(419, 'discussion')\n",
      "(420, 'discussionRecent')\n",
      "(421, 'disdain')\n",
      "(422, 'disdained')\n",
      "(423, 'disease')\n",
      "(424, 'disguised')\n",
      "(425, 'display')\n",
      "(426, 'dissatisfied')\n",
      "(427, 'distinguished')\n",
      "(428, 'distract')\n",
      "(429, 'divert')\n",
      "(430, 'do')\n",
      "(431, 'doesn')\n",
      "(432, 'doing')\n",
      "(433, 'domain')\n",
      "(434, 'domainPublic')\n",
      "(435, 'domainfalsefalse')\n",
      "(436, 'domestic')\n",
      "(437, 'don')\n",
      "(438, 'done')\n",
      "(439, 'donkey')\n",
      "(440, 'down')\n",
      "(441, 'dozen')\n",
      "(442, 'dragged')\n",
      "(443, 'drawing-room')\n",
      "(444, 'drawing-rooms')\n",
      "(445, 'drawn')\n",
      "(446, 'dress-closets')\n",
      "(447, 'drew')\n",
      "(448, 'dropped')\n",
      "(449, 'each')\n",
      "(450, 'earth')\n",
      "(451, 'ease')\n",
      "(452, 'easel')\n",
      "(453, 'easy')\n",
      "(454, 'echoed')\n",
      "(455, 'economy')\n",
      "(456, 'edited')\n",
      "(457, 'effect')\n",
      "(458, 'effects')\n",
      "(459, 'efforts')\n",
      "(460, 'egregious')\n",
      "(461, 'eighteenth-century')\n",
      "(462, 'elbow')\n",
      "(463, 'elegant')\n",
      "(464, 'else')\n",
      "(465, 'embarrassed')\n",
      "(466, 'enabled')\n",
      "(467, 'end')\n",
      "(468, 'endless')\n",
      "(469, 'enjoy')\n",
      "(470, 'enlightenment')\n",
      "(471, 'enough')\n",
      "(472, 'ensuing')\n",
      "(473, 'equally')\n",
      "(474, 'equanimity')\n",
      "(475, 'escape')\n",
      "(476, 'established')\n",
      "(477, 'etching')\n",
      "(478, 'even')\n",
      "(479, 'event')\n",
      "(480, 'ever')\n",
      "(481, 'everlasting')\n",
      "(482, 'every')\n",
      "(483, 'exasperated')\n",
      "(484, 'except')\n",
      "(485, 'excuse')\n",
      "(486, 'excusing')\n",
      "(487, 'existed')\n",
      "(488, 'expected')\n",
      "(489, 'exquisite')\n",
      "(490, 'exquisitely')\n",
      "(491, 'extenuation')\n",
      "(492, 'exterminating')\n",
      "(493, 'extracting')\n",
      "(494, 'eye')\n",
      "(495, 'eyebrows')\n",
      "(496, 'eyes')\n",
      "(497, 'face')\n",
      "(498, 'faces')\n",
      "(499, 'fact')\n",
      "(500, 'faded')\n",
      "(501, 'failed')\n",
      "(502, 'failure')\n",
      "(503, 'fair')\n",
      "(504, 'faith')\n",
      "(505, 'false')\n",
      "(506, 'familiar')\n",
      "(507, 'famille-verte')\n",
      "(508, 'fancy')\n",
      "(509, 'fashionable')\n",
      "(510, 'fate')\n",
      "(511, 'feather')\n",
      "(512, 'feet')\n",
      "(513, 'fell')\n",
      "(514, 'fellow')\n",
      "(515, 'felt')\n",
      "(516, 'few')\n",
      "(517, 'fewer')\n",
      "(518, 'finality')\n",
      "(519, 'find')\n",
      "(520, 'fingers')\n",
      "(521, 'first')\n",
      "(522, 'fit')\n",
      "(523, 'fitting')\n",
      "(524, 'five')\n",
      "(525, 'flash')\n",
      "(526, 'flashed')\n",
      "(527, 'florid')\n",
      "(528, 'flowers')\n",
      "(529, 'fluently')\n",
      "(530, 'flung')\n",
      "(531, 'follow')\n",
      "(532, 'followed')\n",
      "(533, 'fond')\n",
      "(534, 'footstep')\n",
      "(535, 'for')\n",
      "(536, 'forced')\n",
      "(537, 'forcing')\n",
      "(538, 'forehead')\n",
      "(539, 'foreign')\n",
      "(540, 'foreseen')\n",
      "(541, 'forgive')\n",
      "(542, 'forgotten')\n",
      "(543, 'form')\n",
      "(544, 'formats')\n",
      "(545, 'formed')\n",
      "(546, 'forming')\n",
      "(547, 'forward')\n",
      "(548, 'fostered')\n",
      "(549, 'found')\n",
      "(550, 'foundations')\n",
      "(551, 'fragment')\n",
      "(552, 'fragments')\n",
      "(553, 'frame')\n",
      "(554, 'frames')\n",
      "(555, 'frequently')\n",
      "(556, 'friend')\n",
      "(557, 'from')\n",
      "(558, 'full')\n",
      "(559, 'fullest')\n",
      "(560, 'furiously')\n",
      "(561, 'furrowed')\n",
      "(562, 'garlanded')\n",
      "(563, 'garlands')\n",
      "(564, 'gave')\n",
      "(565, 'genial')\n",
      "(566, 'genius')\n",
      "(567, 'gesture')\n",
      "(568, 'get')\n",
      "(569, 'getting')\n",
      "(570, 'give')\n",
      "(571, 'given')\n",
      "(572, 'glad')\n",
      "(573, 'glanced')\n",
      "(574, 'glimpse')\n",
      "(575, 'gloried')\n",
      "(576, 'glory')\n",
      "(577, 'go')\n",
      "(578, 'going')\n",
      "(579, 'gone')\n",
      "(580, 'good')\n",
      "(581, 'good-breeding')\n",
      "(582, 'good-humoured')\n",
      "(583, 'got')\n",
      "(584, 'grace')\n",
      "(585, 'gradually')\n",
      "(586, 'gray')\n",
      "(587, 'grayish')\n",
      "(588, 'great')\n",
      "(589, 'greatest')\n",
      "(590, 'greatness')\n",
      "(591, 'grew')\n",
      "(592, 'groping')\n",
      "(593, 'growing')\n",
      "(594, 'had')\n",
      "(595, 'hadn')\n",
      "(596, 'hair')\n",
      "(597, 'half')\n",
      "(598, 'half-light')\n",
      "(599, 'half-mechanically')\n",
      "(600, 'hall')\n",
      "(601, 'hand')\n",
      "(602, 'hands')\n",
      "(603, 'handsome')\n",
      "(604, 'hanging')\n",
      "(605, 'happen')\n",
      "(606, 'happened')\n",
      "(607, 'hard')\n",
      "(608, 'hardly')\n",
      "(609, 'has')\n",
      "(610, 'have')\n",
      "(611, 'haven')\n",
      "(612, 'having')\n",
      "(613, 'he')\n",
      "(614, 'head')\n",
      "(615, 'hear')\n",
      "(616, 'heard')\n",
      "(617, 'heart')\n",
      "(618, 'height')\n",
      "(619, 'her')\n",
      "(620, 'here')\n",
      "(621, 'hereRelated')\n",
      "(622, 'hermit')\n",
      "(623, 'herself')\n",
      "(624, 'hesitations')\n",
      "(625, 'hide')\n",
      "(626, 'high')\n",
      "(627, 'him')\n",
      "(628, 'himself')\n",
      "(629, 'hint')\n",
      "(630, 'his')\n",
      "(631, 'history')\n",
      "(632, 'holding')\n",
      "(633, 'home')\n",
      "(634, 'honour')\n",
      "(635, 'hooded')\n",
      "(636, 'hostess')\n",
      "(637, 'hot-house')\n",
      "(638, 'hour')\n",
      "(639, 'hours')\n",
      "(640, 'house')\n",
      "(641, 'how')\n",
      "(642, 'https')\n",
      "(643, 'hung')\n",
      "(644, 'husband')\n",
      "(645, 'idea')\n",
      "(646, 'idle')\n",
      "(647, 'idling')\n",
      "(648, 'if')\n",
      "(649, 'immediately')\n",
      "(650, 'in')\n",
      "(651, 'inTalkContributionsCreate')\n",
      "(652, 'incense')\n",
      "(653, 'indexAuthorsRandom')\n",
      "(654, 'indifferent')\n",
      "(655, 'inevitable')\n",
      "(656, 'inevitably')\n",
      "(657, 'inflexible')\n",
      "(658, 'informationCite')\n",
      "(659, 'insensible')\n",
      "(660, 'insignificant')\n",
      "(661, 'instinctively')\n",
      "(662, 'instructive')\n",
      "(663, 'interesting')\n",
      "(664, 'into')\n",
      "(665, 'ironic')\n",
      "(666, 'irony')\n",
      "(667, 'irrelevance')\n",
      "(668, 'irrevocable')\n",
      "(669, 'is')\n",
      "(670, 'it')\n",
      "(671, 'its')\n",
      "(672, 'itself')\n",
      "(673, 'jardiniere')\n",
      "(674, 'jealousy')\n",
      "(675, 'just')\n",
      "(676, 'keep')\n",
      "(677, 'kept')\n",
      "(678, 'key')\n",
      "(679, 'kind')\n",
      "(680, 'knees')\n",
      "(681, 'knew')\n",
      "(682, 'know')\n",
      "(683, 'known')\n",
      "(684, 'laid')\n",
      "(685, 'lair')\n",
      "(686, 'landing')\n",
      "(687, 'language')\n",
      "(688, 'languages')\n",
      "(689, 'last')\n",
      "(690, 'late')\n",
      "(691, 'later')\n",
      "(692, 'latter')\n",
      "(693, 'laugh')\n",
      "(694, 'laughed')\n",
      "(695, 'lay')\n",
      "(696, 'leading')\n",
      "(697, 'lean')\n",
      "(698, 'learned')\n",
      "(699, 'least')\n",
      "(700, 'leathery')\n",
      "(701, 'leave')\n",
      "(702, 'led')\n",
      "(703, 'left')\n",
      "(704, 'leisure')\n",
      "(705, 'lends')\n",
      "(706, 'lent')\n",
      "(707, 'let')\n",
      "(708, 'lies')\n",
      "(709, 'life')\n",
      "(710, 'life-likeness')\n",
      "(711, 'lift')\n",
      "(712, 'lifted')\n",
      "(713, 'light')\n",
      "(714, 'lightly')\n",
      "(715, 'like')\n",
      "(716, 'liked')\n",
      "(717, 'line')\n",
      "(718, 'lines')\n",
      "(719, 'lingered')\n",
      "(720, 'linkPage')\n",
      "(721, 'links')\n",
      "(722, 'lips')\n",
      "(723, 'lit')\n",
      "(724, 'little')\n",
      "(725, 'live')\n",
      "(726, 'll')\n",
      "(727, 'loathing')\n",
      "(728, 'logged')\n",
      "(729, 'long')\n",
      "(730, 'longed')\n",
      "(731, 'longer')\n",
      "(732, 'look')\n",
      "(733, 'looked')\n",
      "(734, 'looking')\n",
      "(735, 'lose')\n",
      "(736, 'loss')\n",
      "(737, 'lounging')\n",
      "(738, 'lovely')\n",
      "(739, 'lucky')\n",
      "(740, 'lump')\n",
      "(741, 'luncheon-table')\n",
      "(742, 'luxury')\n",
      "(743, 'lying')\n",
      "(744, 'made')\n",
      "(745, 'make')\n",
      "(746, 'man')\n",
      "(747, 'manage')\n",
      "(748, 'managed')\n",
      "(749, 'mantel-piece')\n",
      "(750, 'marble')\n",
      "(751, 'married')\n",
      "(752, 'may')\n",
      "(753, 'me')\n",
      "(754, 'meant')\n",
      "(755, 'mediocrity')\n",
      "(756, 'medium')\n",
      "(757, 'mentioned')\n",
      "(758, 'menu')\n",
      "(759, 'mere')\n",
      "(760, 'merely')\n",
      "(761, 'met')\n",
      "(762, 'might')\n",
      "(763, 'mighty')\n",
      "(764, 'millionaire')\n",
      "(765, 'mine')\n",
      "(766, 'minute')\n",
      "(767, 'minutes')\n",
      "(768, 'mirrors')\n",
      "(769, 'modest')\n",
      "(770, 'modesty')\n",
      "(771, 'moment')\n",
      "(772, 'money')\n",
      "(773, 'monumental')\n",
      "(774, 'mood')\n",
      "(775, 'morbidly')\n",
      "(776, 'more')\n",
      "(777, 'most')\n",
      "(778, 'mourn')\n",
      "(779, 'mourned')\n",
      "(780, 'moustache')\n",
      "(781, 'moved')\n",
      "(782, 'much')\n",
      "(783, 'muddling')\n",
      "(784, 'multiplied')\n",
      "(785, 'murmur')\n",
      "(786, 'muscles')\n",
      "(787, 'must')\n",
      "(788, 'my')\n",
      "(789, 'myself')\n",
      "(790, 'mysterious')\n",
      "(791, 'naive')\n",
      "(792, 'native')\n",
      "(793, 'near')\n",
      "(794, 'nearly')\n",
      "(795, 'negatived')\n",
      "(796, 'nervous')\n",
      "(797, 'nervousness')\n",
      "(798, 'neutral')\n",
      "(799, 'never')\n",
      "(800, 'next')\n",
      "(801, 'no')\n",
      "(802, 'none')\n",
      "(803, 'not')\n",
      "(804, 'note')\n",
      "(805, 'nothing')\n",
      "(806, 'now')\n",
      "(807, 'nymphs')\n",
      "(808, 'oak')\n",
      "(809, 'obituary')\n",
      "(810, 'object')\n",
      "(811, 'objects')\n",
      "(812, 'occurred')\n",
      "(813, 'oddly')\n",
      "(814, 'of')\n",
      "(815, 'off')\n",
      "(816, 'often')\n",
      "(817, 'oh')\n",
      "(818, 'old')\n",
      "(819, 'on')\n",
      "(820, 'once')\n",
      "(821, 'one')\n",
      "(822, 'ones')\n",
      "(823, 'only')\n",
      "(824, 'onto')\n",
      "(825, 'open')\n",
      "(826, 'or')\n",
      "(827, 'org/w/index')\n",
      "(828, 'other')\n",
      "(829, 'our')\n",
      "(830, 'ourselves')\n",
      "(831, 'out')\n",
      "(832, 'outline')\n",
      "(833, 'oval')\n",
      "(834, 'over')\n",
      "(835, 'own')\n",
      "(836, 'packed')\n",
      "(837, 'page')\n",
      "(838, 'pageGet')\n",
      "(839, 'pagesPermanent')\n",
      "(840, 'paid')\n",
      "(841, 'paint')\n",
      "(842, 'painted')\n",
      "(843, 'painter')\n",
      "(844, 'painting')\n",
      "(845, 'pale')\n",
      "(846, 'paled')\n",
      "(847, 'palm-trees')\n",
      "(848, 'panel')\n",
      "(849, 'panelling')\n",
      "(850, 'pardonable')\n",
      "(851, 'pardoned')\n",
      "(852, 'part')\n",
      "(853, 'passages')\n",
      "(854, 'passing')\n",
      "(855, 'past')\n",
      "(856, 'pastels')\n",
      "(857, 'pathos')\n",
      "(858, 'patient')\n",
      "(859, 'people')\n",
      "(860, 'perceptible')\n",
      "(861, 'perfect')\n",
      "(862, 'persistence')\n",
      "(863, 'persuasively')\n",
      "(864, 'php')\n",
      "(865, 'phrase')\n",
      "(866, 'picture')\n",
      "(867, 'pictures')\n",
      "(868, 'pines')\n",
      "(869, 'pink')\n",
      "(870, 'place')\n",
      "(871, 'placed')\n",
      "(872, 'plain')\n",
      "(873, 'platitudes')\n",
      "(874, 'pleased')\n",
      "(875, 'pockets')\n",
      "(876, 'point')\n",
      "(877, 'poised')\n",
      "(878, 'policy')\n",
      "(879, 'poor')\n",
      "(880, 'portalCentral')\n",
      "(881, 'portrait')\n",
      "(882, 'posing')\n",
      "(883, 'possessed')\n",
      "(884, 'poverty')\n",
      "(885, 'predicted')\n",
      "(886, 'preliminary')\n",
      "(887, 'presenting')\n",
      "(888, 'prestidigitation')\n",
      "(889, 'pretty')\n",
      "(890, 'previous')\n",
      "(891, 'price')\n",
      "(892, 'pride')\n",
      "(893, 'princely')\n",
      "(894, 'prism')\n",
      "(895, 'problem')\n",
      "(896, 'proclaiming')\n",
      "(897, 'prodigious')\n",
      "(898, 'profusion')\n",
      "(899, 'projects')\n",
      "(900, 'protest')\n",
      "(901, 'prove')\n",
      "(902, 'public')\n",
      "(903, 'published')\n",
      "(904, 'purblind')\n",
      "(905, 'purely')\n",
      "(906, 'pushed')\n",
      "(907, 'put')\n",
      "(908, 'qualities')\n",
      "(909, 'quality')\n",
      "(910, 'queerly')\n",
      "(911, 'question')\n",
      "(912, 'quickly')\n",
      "(913, 'quietly')\n",
      "(914, 'quite')\n",
      "(915, 'quote')\n",
      "(916, 'rain')\n",
      "(917, 'raised')\n",
      "(918, 'random')\n",
      "(919, 'rather')\n",
      "(920, 're')\n",
      "(921, 'real')\n",
      "(922, 'really')\n",
      "(923, 'reared')\n",
      "(924, 'reason')\n",
      "(925, 'reassurance')\n",
      "(926, 'recovering')\n",
      "(927, 'recreated')\n",
      "(928, 'reflected')\n",
      "(929, 'reflection')\n",
      "(930, 'regrets')\n",
      "(931, 'relatively')\n",
      "(932, 'remained')\n",
      "(933, 'remember')\n",
      "(934, 'reminded')\n",
      "(935, 'repeating')\n",
      "(936, 'represented')\n",
      "(937, 'reproduction')\n",
      "(938, 'resented')\n",
      "(939, 'resolve')\n",
      "(940, 'resources')\n",
      "(941, 'rest')\n",
      "(942, 'rich')\n",
      "(943, 'ridiculous')\n",
      "(944, 'robbed')\n",
      "(945, 'romantic')\n",
      "(946, 'room')\n",
      "(947, 'rose')\n",
      "(948, 'rs')\n",
      "(949, 'rule')\n",
      "(950, 'run')\n",
      "(951, 's')\n",
      "(952, 'said')\n",
      "(953, 'same')\n",
      "(954, 'satisfaction')\n",
      "(955, 'savour')\n",
      "(956, 'saw')\n",
      "(957, 'say')\n",
      "(958, 'saying')\n",
      "(959, 'says')\n",
      "(960, 'scorn')\n",
      "(961, 'scornful')\n",
      "(962, 'secret')\n",
      "(963, 'see')\n",
      "(964, 'seemed')\n",
      "(965, 'seen')\n",
      "(966, 'self-confident')\n",
      "(967, 'send')\n",
      "(968, 'sensation')\n",
      "(969, 'sensitive')\n",
      "(970, 'sent')\n",
      "(971, 'serious')\n",
      "(972, 'set')\n",
      "(973, 'sex')\n",
      "(974, 'shade')\n",
      "(975, 'shaking')\n",
      "(976, 'shall')\n",
      "(977, 'she')\n",
      "(978, 'shirked')\n",
      "(979, 'short')\n",
      "(980, 'shortened')\n",
      "(981, 'shorter')\n",
      "(982, 'should')\n",
      "(983, 'shoulder')\n",
      "(984, 'shoulders')\n",
      "(985, 'show')\n",
      "(986, 'showed')\n",
      "(987, 'showy')\n",
      "(988, 'shrug')\n",
      "(989, 'shrugged')\n",
      "(990, 'sight')\n",
      "(991, 'sign')\n",
      "(992, 'silent')\n",
      "(993, 'silver')\n",
      "(994, 'similar')\n",
      "(995, 'simpleton')\n",
      "(996, 'simplifications')\n",
      "(997, 'simply')\n",
      "(998, 'since')\n",
      "(999, 'single')\n",
      "(1000, 'site')\n",
      "(1001, 'sitter')\n",
      "(1002, 'sitters')\n",
      "(1003, 'sketch')\n",
      "(1004, 'skill')\n",
      "(1005, 'slight')\n",
      "(1006, 'slightly')\n",
      "(1007, 'slowly')\n",
      "(1008, 'small')\n",
      "(1009, 'smile')\n",
      "(1010, 'smiling')\n",
      "(1011, 'sneer')\n",
      "(1012, 'so')\n",
      "(1013, 'solace')\n",
      "(1014, 'some')\n",
      "(1015, 'somebody')\n",
      "(1016, 'something')\n",
      "(1017, 'spacious')\n",
      "(1018, 'spaniel')\n",
      "(1019, 'speaking-tubes')\n",
      "(1020, 'speculations')\n",
      "(1021, 'spite')\n",
      "(1022, 'splash')\n",
      "(1023, 'square')\n",
      "(1024, 'stairs')\n",
      "(1025, 'stammer')\n",
      "(1026, 'stand')\n",
      "(1027, 'standing')\n",
      "(1028, 'started')\n",
      "(1029, 'statement')\n",
      "(1030, 'stay')\n",
      "(1031, 'still')\n",
      "(1032, 'stocked')\n",
      "(1033, 'stood')\n",
      "(1034, 'stopped')\n",
      "(1035, 'stopping')\n",
      "(1036, 'straddling')\n",
      "(1037, 'straight')\n",
      "(1038, 'strain')\n",
      "(1039, 'straining')\n",
      "(1040, 'strange')\n",
      "(1041, 'straw')\n",
      "(1042, 'stream')\n",
      "(1043, 'stroke')\n",
      "(1044, 'strokes')\n",
      "(1045, 'strolled')\n",
      "(1046, 'strongest')\n",
      "(1047, 'strongly')\n",
      "(1048, 'struck')\n",
      "(1049, 'studio')\n",
      "(1050, 'stuff')\n",
      "(1051, 'subject')\n",
      "(1052, 'substantial')\n",
      "(1053, 'suburban')\n",
      "(1054, 'such')\n",
      "(1055, 'suddenly')\n",
      "(1056, 'suffered')\n",
      "(1057, 'sugar')\n",
      "(1058, 'suggested')\n",
      "(1059, 'sunburn')\n",
      "(1060, 'sunburnt')\n",
      "(1061, 'sunlit')\n",
      "(1062, 'superb')\n",
      "(1063, 'sure')\n",
      "(1064, 'surest')\n",
      "(1065, 'surface')\n",
      "(1066, 'surprise')\n",
      "(1067, 'surprised')\n",
      "(1068, 'surrounded')\n",
      "(1069, 'suspected')\n",
      "(1070, 'sweetly')\n",
      "(1071, 'sweetness')\n",
      "(1072, 'swelling')\n",
      "(1073, 'swept')\n",
      "(1074, 'swum')\n",
      "(1075, 't')\n",
      "(1076, 'table')\n",
      "(1077, 'take')\n",
      "(1078, 'taken')\n",
      "(1079, 'talking')\n",
      "(1080, 'tea')\n",
      "(1081, 'tears')\n",
      "(1082, 'technicalities')\n",
      "(1083, 'technique')\n",
      "(1084, 'tell')\n",
      "(1085, 'tells')\n",
      "(1086, 'tempting')\n",
      "(1087, 'term')\n",
      "(1088, 'terms')\n",
      "(1089, 'terra-cotta')\n",
      "(1090, 'terrace')\n",
      "(1091, 'terraces')\n",
      "(1092, 'terribly')\n",
      "(1093, 'than')\n",
      "(1094, 'that')\n",
      "(1095, 'the')\n",
      "(1096, 'their')\n",
      "(1097, 'them')\n",
      "(1098, 'then')\n",
      "(1099, 'there')\n",
      "(1100, 'therefore')\n",
      "(1101, 'they')\n",
      "(1102, 'thin')\n",
      "(1103, 'thing')\n",
      "(1104, 'things')\n",
      "(1105, 'think')\n",
      "(1106, 'this')\n",
      "(1107, 'thither')\n",
      "(1108, 'those')\n",
      "(1109, 'though')\n",
      "(1110, 'thought')\n",
      "(1111, 'three')\n",
      "(1112, 'threshold')\n",
      "(1113, 'threw')\n",
      "(1114, 'through')\n",
      "(1115, 'throwing')\n",
      "(1116, 'tie')\n",
      "(1117, 'till')\n",
      "(1118, 'time')\n",
      "(1119, 'timorously')\n",
      "(1120, 'tinge')\n",
      "(1121, 'tips')\n",
      "(1122, 'tired')\n",
      "(1123, 'title=The')\n",
      "(1124, 'to')\n",
      "(1125, 'told')\n",
      "(1126, 'tone')\n",
      "(1127, 'tones')\n",
      "(1128, 'too')\n",
      "(1129, 'took')\n",
      "(1130, 'tools')\n",
      "(1131, 'tottering')\n",
      "(1132, 'touched')\n",
      "(1133, 'toward')\n",
      "(1134, 'trace')\n",
      "(1135, 'trade')\n",
      "(1136, 'transcriptionHelpDonate')\n",
      "(1137, 'transmute')\n",
      "(1138, 'traps')\n",
      "(1139, 'travelled')\n",
      "(1140, 'tribute')\n",
      "(1141, 'tributes')\n",
      "(1142, 'tricks')\n",
      "(1143, 'tried')\n",
      "(1144, 'trouser-presses')\n",
      "(1145, 'true')\n",
      "(1146, 'truth')\n",
      "(1147, 'turned')\n",
      "(1148, 'twenty')\n",
      "(1149, 'twenty-four')\n",
      "(1150, 'twice')\n",
      "(1151, 'twirling')\n",
      "(1152, 'unaccountable')\n",
      "(1153, 'uncertain')\n",
      "(1154, 'under')\n",
      "(1155, 'underlay')\n",
      "(1156, 'underneath')\n",
      "(1157, 'understand')\n",
      "(1158, 'unexpected')\n",
      "(1159, 'untouched')\n",
      "(1160, 'unusual')\n",
      "(1161, 'up')\n",
      "(1162, 'up-stream')\n",
      "(1163, 'upon')\n",
      "(1164, 'upset')\n",
      "(1165, 'upstairs')\n",
      "(1166, 'us')\n",
      "(1167, 'used')\n",
      "(1168, 'using')\n",
      "(1169, 'usual')\n",
      "(1170, 'value')\n",
      "(1171, 'varnishing')\n",
      "(1172, 'vases')\n",
      "(1173, 've')\n",
      "(1174, 'veins')\n",
      "(1175, 'velveteen')\n",
      "(1176, 'versionDownload')\n",
      "(1177, 'very')\n",
      "(1178, 'view')\n",
      "(1179, 'villa')\n",
      "(1180, 'vindicated')\n",
      "(1181, 'virtuosity')\n",
      "(1182, 'vista')\n",
      "(1183, 'vocation')\n",
      "(1184, 'voice')\n",
      "(1185, 'wall')\n",
      "(1186, 'wander')\n",
      "(1187, 'want')\n",
      "(1188, 'wanted')\n",
      "(1189, 'wants')\n",
      "(1190, 'was')\n",
      "(1191, 'wasn')\n",
      "(1192, 'watched')\n",
      "(1193, 'watching')\n",
      "(1194, 'water-colour')\n",
      "(1195, 'waves')\n",
      "(1196, 'way')\n",
      "(1197, 'weekly')\n",
      "(1198, 'weeks')\n",
      "(1199, 'welcome')\n",
      "(1200, 'went')\n",
      "(1201, 'were')\n",
      "(1202, 'what')\n",
      "(1203, 'when')\n",
      "(1204, 'whenever')\n",
      "(1205, 'where')\n",
      "(1206, 'which')\n",
      "(1207, 'while')\n",
      "(1208, 'white')\n",
      "(1209, 'white-panelled')\n",
      "(1210, 'who')\n",
      "(1211, 'whole')\n",
      "(1212, 'whom')\n",
      "(1213, 'why')\n",
      "(1214, 'wide')\n",
      "(1215, 'widow')\n",
      "(1216, 'wife')\n",
      "(1217, 'wikisource')\n",
      "(1218, 'wild')\n",
      "(1219, 'wincing')\n",
      "(1220, 'window-curtains')\n",
      "(1221, 'wish')\n",
      "(1222, 'with')\n",
      "(1223, 'without')\n",
      "(1224, 'wits')\n",
      "(1225, 'woman')\n",
      "(1226, 'women')\n",
      "(1227, 'won')\n",
      "(1228, 'wonder')\n",
      "(1229, 'wondered')\n",
      "(1230, 'word')\n",
      "(1231, 'work')\n",
      "(1232, 'workRandom')\n",
      "(1233, 'working')\n",
      "(1234, 'works')\n",
      "(1235, 'worksPD-USHidden')\n",
      "(1236, 'worksSpoken')\n",
      "(1237, 'worth')\n",
      "(1238, 'would')\n",
      "(1239, 'wouldn')\n",
      "(1240, 'year')\n",
      "(1241, 'years')\n",
      "(1242, 'yellow')\n",
      "(1243, 'yet')\n",
      "(1244, 'you')\n",
      "(1245, 'younger')\n",
      "(1246, 'your')\n",
      "(1247, 'yourself')\n"
     ]
    }
   ],
   "source": [
    "# convert tokent to tokdn id each unique tokedn and add to the vocabukary. assign an id\n",
    "vocab = {token: idx for idx, token in enumerate(sorted(set(preprocessed)))} #maping word to integer\n",
    "for i in enumerate(sorted(set(preprocessed))):\n",
    "    print(i) #enumerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 0)\n",
      "(' ', 1)\n",
      "('!', 2)\n",
      "('\"', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "(',', 7)\n",
      "('--', 8)\n",
      "('.', 9)\n",
      "('//en', 10)\n",
      "('03', 11)\n",
      "('1', 12)\n",
      "('18', 13)\n",
      "('1908', 14)\n",
      "('1929', 15)\n",
      "('2', 16)\n",
      "('2021', 17)\n",
      "(':', 18)\n",
      "(';', 19)\n",
      "('?', 20)\n",
      "('A', 21)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >20:\n",
    "        break #enumerate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Token ids \n",
    "depending on the formation of word in a sentance and using vocabulary gives token ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode text (text to token\n",
    "# tocken to token ids\n",
    "class SimpleTokenizerV1:\n",
    "\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab  # will store the vocabulaty string to integer mapping\n",
    "        self.int_to_str = {i:s for s, i in vocab.items()} #reverse mapping (integer :: strubg)\n",
    "    \n",
    "    def encode(self, text): # converts text to token ids\n",
    "        preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        text = re.sub(r'\\s+([,.:;?_!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[79, 69, 212, 1110, 83, 63, 919, 175, 327, 566, 8, 1109, 175]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"I HAD always thought Jack Gisburn rather a cheap genius--though a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I HAD always thought Jack Gisburn rather a cheap genius -- though a'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([79, 69, 212, 1110, 83, 63, 919, 175, 327, 566, 8, 1109, 175])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Byte Pair encoding\n",
    "creating new llm there is training of tokenizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handles - unknown word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken (using fast ai python library but, through rust) 6x faster than hugging dace tokenizeer\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"}) # another token called endoftoken to donate when text ends and when int begins. \n",
    "\n",
    "print(integers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)\n",
    "# can handle unknon word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[461, 10456, 71, 69, 74, 8457, 24411, 69, 41582]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "integers = tokenizer.encode(\"akjudhfkjsadhfkl\") # tokenize unknow text\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "akjudhfkjsadhfkl\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "js\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode([8457])\n",
    "print(strings)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if model has seen words like \"as\", \"dk\" then it will use two character token. breaks into sub-parts. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'supplementary'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-64315687dc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msupplementary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_dataloader_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'supplementary'"
     ]
    }
   ],
   "source": [
    "from supplementary import create_dataloader_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i:i + max_length]\n",
    "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, \n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from supplementary import create_dataloader_v1\n",
    "\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False) # only 4 tokens per row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[  198,    40,   367,  2885],\n",
      "        [ 1464,  1807,  3619,   402],\n",
      "        [  271, 10899,  2138,   257],\n",
      "        [ 7026, 15632,   438,  2016],\n",
      "        [  257,   198, 11274,  5891],\n",
      "        [ 1576,   438,   568,   340],\n",
      "        [  373,   645,  1049,  5975],\n",
      "        [  284,   502,   284,  3285]])\n",
      "\n",
      "Targets:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  198, 11274,  5891,  1576],\n",
      "        [  438,   568,   340,   373],\n",
      "        [  645,  1049,  5975,   284],\n",
      "        [  502,   284,  3285,   326]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets) # each training exaples consists of 4 words. in a batch of 8 \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are tokenized text -> frd into embedding layers -> transformer blocl. mostly llms work in same way be it  large or small.\n",
    "embeddin - similarity/ cosine swapping the activation funtion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# em\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,    # Vocabulary size - unique word v\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"emb_dim\": 768,         # Embedding dimension\n",
    "    \"n_heads\": 12,          # Number of attention heads\n",
    "    \"n_layers\": 12,         # Number of layers\n",
    "    \"drop_rate\": 0.0,       # Dropout rate\n",
    "    \"qkv_bias\": False       # Query-Key-Value bias masked multi headed attenstion moden artich linea layer is matrix multiplciation. qkv matrix. instead of linear transformation \n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output of the out layer is tensor. (nRows = nWords)# columns = volcabulary. Looking for highest value in the row. corresponding to token_id where there is max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # linear layers to project input features to query, key, and value\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # createsc casual / autoregressive mask to prevent attending to future tokens in the sequence  # upper triangular matrix   = -inf\n",
    "#d_in: Dimension of the input features.\n",
    "#d_out: Dimension of the output features.\n",
    "#context_length: The maximum sequence length for the context.\n",
    "#dropout: Dropout rate for the attention weights.\n",
    "#num_heads: Number of attention heads.\n",
    "#qkv_bias: Boolean indicating whether to include a bias term in the linear layers.\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # b : is batch siuze\n",
    "\n",
    "        # Linear Transformations\n",
    "        keys = self.W_key(x)  # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "# Reshape for multi-head attention\n",
    "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
    "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "# Applies softmax to convert scores to probabilities, scaled by the square root of the head dimensio\n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "# applies droput to attention weights\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        # compute weighted sum of values based on attention weights and trnaspose the result\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
    "\n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)  # optional projection\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [[\n",
    "    [1, 0, 1, 0],   # Token 1\n",
    "    [0, 1, 0, 1]    # Token 2\n",
    "]]\n",
    "# X with shape (batch_size, num_tokens, d_in) = (1, 2, 4), where batch_size = 1, num_tokens = 2, and d_in = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = keys = values = [[\n",
    "    [[1, 0], [1, 0]],   # Head 1\n",
    "    [[0, 1], [0, 1]]    # Head 2\n",
    "]]\n",
    "# le d_in = d_out = 4 W_query = W_key = W_value = [[1, 0, 0, 0], [0, 1, 0, 0], [0, 0, 1, 0], [0, 0, 0, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input X (Shape: [1, 2, 4])\n",
    "#  ┌──────────┬──────────┐\n",
    "#  │ 1, 0, 1, 0 │ 0, 1, 0, 1 │\n",
    "#  └──────────┴──────────┘\n",
    "#       |            |\n",
    "#       ▼            ▼\n",
    "#  Queries, Keys, Values (after linear projection)\n",
    "#       ┌────────────────────────┐\n",
    "#       │     Head 1     │     Head 2    │\n",
    "#       ├───────────┼───────────┤\n",
    "#       │ 1, 0       │ 0, 1       │\n",
    "#       │ 1, 0       │ 0, 1       │\n",
    "#       └───────────┴───────────┘\n",
    "#       |            |\n",
    "#       ▼            ▼\n",
    "#  Attention Scores (e.g., dot product of queries and keys)\n",
    "#       ┌────────────────────────┐\n",
    "#       │     Head 1     │     Head 2    │\n",
    "#       ├───────────┼───────────┤\n",
    "#       │ 1, 0       │ 0, 1       │\n",
    "#       │ 1, 0       │ 0, 1       │\n",
    "#       └───────────┴───────────┘\n",
    "#       |            |\n",
    "#       ▼            ▼\n",
    "#  Attention Weights (after softmax)\n",
    "#       ┌────────────────────────┐\n",
    "#       │     Head 1     │     Head 2    │\n",
    "#       ├───────────┼───────────┤\n",
    "#       │ 0.5, 0.5   │ 0.5, 0.5   │\n",
    "#       │ 0.5, 0.5   │ 0.5, 0.5   │\n",
    "#       └───────────┴───────────┘\n",
    "#       |            |\n",
    "#       ▼            ▼\n",
    "#  Context Vectors (weighted sum of values)\n",
    "#       ┌────────────────────────┐\n",
    "#       │     Head 1     │     Head 2    │\n",
    "#       ├───────────┼───────────┤\n",
    "#       │ 0.5, 0.5   │ 0.5, 0.5   │\n",
    "#       │ 0.5, 0.5   │ 0.5, 0.5   │\n",
    "#       └───────────┴───────────┘\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GELU(nn.Module): # Gaussian Error Linear unit GELU(x)=0.5⋅x⋅(1+tanh(π2​​⋅(x+0.044715⋅x3))) introduces non-linearity in a smooth, differentiable manner, which helps neural networks learn complex patterns more effectively\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class provides a simple yet effective architecture for transforming and processing input embeddings in neural networks.\n",
    "#h1​=W1​x+b1​ where W1W1​ is a weight matrix of size (4 \\times \\text{emb_dim}) \\times \\text{emb_dim} and b1b1​ is a bias vector of size 4 \\times \\text{emb_dim}.\n",
    "\n",
    "#GELU Activation: h2=GELU(h1)=0.5⋅h1⋅(1+tanh⁡(2π⋅(h1+0.044715⋅h13)))h2​=GELU(h1​)=0.5⋅h1​⋅(1+tanh(π2​⋅(h1​+0.044715⋅h13​)))\n",
    "\n",
    "#Second Linear Transformation: y=W2h2+b2y=W2​h2​+b2​ where W2W2​ \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
